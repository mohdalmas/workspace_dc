apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "hadoop.fullname" . }}
  labels:
    app: {{ include "hadoop.name" . }}
    chart: {{ include "hadoop.chart" . }}
    release: {{ .Release.Name }}
data:
    
  datanode-monitor.sh: |
    code=$(curl -s -o /dev/null -w "%{http_code}" 'http://localhost:9864')
    if [[ $code != 200 || $code != 503 ]]; then
        $HADOOP_HOME/bin/hdfs --daemon start datanode
    else
        echo DataNode Service is currently running and will not be started.
    fi
    
    
  bootstrap.sh: |
    #!/bin/bash
  
    #Hadoop Env initialize 
    : ${HADOOP_HOME:=/opt/hadoop}    
    . $HADOOP_HOME/etc/hadoop/hadoop-env.sh

    CONFIG_DIR="/tmp/hadoop-config"

    # Copy hadoop config files from volume mount
    for f in slaves core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml capacity-scheduler.xml datanode-monitor.sh krb5.conf ssl-server.xml ; do
      if [[ -e ${CONFIG_DIR}/$f ]]; then
        #if [[ f == "krb5.conf" ]];then
        #  cp ${CONFIG_DIR}/$f /etc/$f
          cat ${CONFIG_DIR}/krb5.conf > /etc/krb5.conf
        
          cp ${CONFIG_DIR}/$f $HADOOP_HOME/etc/hadoop/$f
        
      else
        echo "ERROR: Could not find $f in $CONFIG_DIR"
        exit 1
      fi
    done
    

    # installing libraries if any - (resource urls added comma separated to the ACP system variable)
    cd $HADOOP_HOME/share/hadoop/common ; for cp in ${ACP//,/ }; do  echo == $cp; curl -LO $cp ; done; cd -
    
    #groupadd -r -g 499 hdpusers
    useradd --comment "Hive user" -u 500 --shell /bin/bash -M -r -g hdpusers hive
    #useradd --comment "HBase user" -u 501 --shell /bin/bash -M -r -g hdpusers hbase
    useradd --comment "HDFS user" -u 502 --shell /bin/bash -M -r -g hdpusers hdfs
    useradd --comment "Spark user" -u 503 --shell /bin/bash -M -r -g hdpusers spark
    useradd --comment "Yarn user" -u 504 --shell /bin/bash -M -r -g hdpusers yarn
    useradd --comment "Yarn ATS user" -u 505 --shell /bin/bash -M -r -g hdpusers yarn-ats
    useradd --comment "Knox user" -u 506 --shell /bin/bash -M -r -g hdpusers knox
    useradd --comment "MAPR user" -u 507 --shell /bin/bash -M -r -g hdpusers mapred
    useradd --comment "Kafka user" -u 508 --shell /bin/bash -M -r -g hdpusers kafka
    useradd --comment "Zookeeper user" -u 509 --shell /bin/bash -M -r -g hdpusers zookeeper
    useradd --comment "Ranger user" -u 510 --shell /bin/bash -M -r -g hdpusers ranger
    useradd --comment "Atlas user" -u 511 --shell /bin/bash -M -r -g hdpusers atlas
    useradd --comment "Tez user" -u 512 --shell /bin/bash -M -r -g hdpusers tez
    useradd --comment "KMS user" -u 513 --shell /bin/bash -M -r -g hdpusers kms
    useradd --comment "Oozie user" -u 514 --shell /bin/bash -M -r -g hdpusers oozie
    useradd --comment "Infra-Solr user" -u 515 --shell /bin/bash -M -r -g hdpusers infra-solr
    useradd --comment "Livy user" -u 516 --shell /bin/bash -M -r -g hdpusers livy
    useradd --comment "Ambari QA user" -u 517 --shell /bin/bash -M -r -g hdpusers ambari-qa
    usermod -a -G hdpusers root
    usermod -a -G hdpusers hive
    usermod -a -G hdpusers hbase
    usermod -a -G hdpusers hdfs
    usermod -a -G hdpusers spark
    usermod -a -G hdpusers yarn
    usermod -a -G hdpusers knox
    usermod -a -G hdpusers mapred
    usermod -a -G hdpusers kafka
    usermod -a -G hdpusers yarn-ats
    usermod -a -G hdpusers zookeeper
    usermod -a -G hdpusers ranger
    usermod -a -G hdpusers atlas
    usermod -a -G hdpusers tez
    usermod -a -G hdpusers kms
    usermod -a -G hdpusers oozie
    usermod -a -G hdpusers infra-solr
    usermod -a -G hdpusers livy
    usermod -a -G hdpusers ambari-qa
    
    chmod 700 /etc/security/keytabs/hadoop.jks
    chown hdfs:hdpusers /etc/security/keytabs/hadoop.jks      
    # mkdir /etc/security/keytabs
    chmod -R 750 /etc/security/keytabs
    # chown -R hdfs:hdpusers /tmp
    cp /tmp/key/hadoop.service.keytab /etc/security/keytabs/
    cp /tmp/key/yarn.service.keytab /etc/security/keytabs/    

    
    if [ ! -d "${HADOOP_HOME}/logs"  ]; then
       mkdir ${HADOOP_HOME}/logs
       
       if [[ "${HOSTNAME}" =~ "hdfs-nn" || "${HOSTNAME}" =~ "hdfs-dn" ]]
       then
         chown -R hdfs:hdpusers /dfs
         chown -R hdfs:hdpusers /opt/hadoop 
         chown -R hdfs:hdpusers /opt/hadoop-3.1.1
         #chmod 700 /etc/security/keytabs/hadoop.jks
         #chmod 700 -R /etc/security/keytabs
         chown hdfs:hdpusers /etc/security/keytabs/hadoop.jks
         chown hdfs:hdpusers /etc/security/keytabs/hadoop.service.keytab
         chown -R hdfs:hdpusers ${HADOOP_HOME}/logs
         
       else
         chown -R yarn:hdpusers ${HADOOP_HOME}/logs
         chown -R hdfs:hdpusers /etc/security/keytabs
       fi
    fi
    
    if [[ "${HOSTNAME}" =~ "hdfs-nn" ]]; then
      
      
      #until kinit -kt /etc/security/keytabs/hadoop.service.keytab hdfs/hadoop@{{ .Values.realm }}; do sleep 15; done

      echo "KDC is up and ready to go... starting up"
      
      # The below format command must run once with installation then it must be comment and upgrade the chart.  
      if [ ! -d "/dfs/name/current" ]; then
         # Take action if /root/hdfs/namenode/current does not exists. #
         $HADOOP_HOME/bin/hdfs namenode -format -force -nonInteractive 
      fi
      
      #Replace the Cluster ID
      sed -i '/clusterID=CID-/c\clusterID=CID-37f8ac39-a345-4119-893f-bd85be359786' /dfs/name/current/VERSION
      
      #Replace the Block Pool ID
      sed -i '/blockpoolID=BP-/c\blockpoolID=BP-506570155-data-1646504148639' /dfs/name/current/VERSION
      
      $HADOOP_HOME/bin/hdfs --daemon start namenode

    fi

    if [[ "${HOSTNAME}" =~ "hdfs-dn" ]]; then
    

      # #  wait up to 30 seconds for namenode
      (while [[ $count -lt 30 && -z `curl -sf http://{{ include "hadoop.fullname" . }}-hdfs-nn:9870` ]]; do ((count=count+1)) ; echo "Waiting for {{ include "hadoop.fullname" . }}-hdfs-nn" ; sleep 2; done && [[ $count -lt 30 ]])
      #[[ $? -ne 0 ]] && echo "Timeout waiting for hdfs-nn, exiting." && exit 1
      
      until kinit -kt /etc/security/keytabs/hadoop.service.keytab hdfs/hadoop@{{ .Values.realm }}; do sleep 15; done

      echo "KDC is up and ready to go... starting up"
      
     #Replace the Cluster ID
      sed -i '/clusterID=CID-/c\clusterID=CID-37f8ac39-a345-4119-893f-bd85be359786' /dfs/data/current/VERSION    
      
      $HADOOP_HOME/bin/hdfs --daemon start datanode
       
      hdfs dfsadmin -safemode leave
      
      # ########Harmful lines########
      # hdfs dfs -chmod -R 755 "/"
      # hdfs dfs -chown root:hdpusers "/"
      
      if [ ! -d "hdfs dfs -ls /tmp" ]; then
         #Create tmp dir
         hdfs dfs -mkdir /tmp
         hdfs dfs -chmod -R 777 /tmp
         hdfs dfs -chmod g+w /tmp
         hdfs dfs -chown -R hive:hdpusers /tmp
      fi
      
      if [ ! -d "hdfs dfs -ls /user" ]; then
         #Create user dir
         hdfs dfs -mkdir /user
         hdfs dfs -chmod -R 777 /user
         hdfs dfs -chmod g+w /user
         hdfs dfs -chown -R root:hdpusers /user
      fi
      
      if [ ! -d "hdfs dfs -ls /hbase" ]; then
         #Create HBase dir
         hdfs dfs -mkdir -p /hbase
         hdfs dfs -chmod -R 700 /hbase
         hdfs dfs -chmod g+w /hbase
         hdfs dfs -chown -R hbase:hbase /hbase
      fi
      
      if [ ! -d "hdfs dfs -ls /hive" ]; then
         #Create Hive dir
         hdfs dfs -mkdir -p /hive/warehouse
         hdfs dfs -chmod -R 700 /hive
         hdfs dfs -chmod g+w /hive
         hdfs dfs -chown -R hive:hive /hive
      fi    
      
      #Monitor DataNodes
      chmod 777 -R /opt/hadoop/etc/hadoop/datanode-monitor.sh
      chmod +x /opt/hadoop/etc/hadoop/datanode-monitor.sh
      while true; do $HADOOP_HOME/etc/hadoop/datanode-monitor.sh; sleep 600; done

    fi

    if [[ "${HOSTNAME}" =~ "yarn-rm" ]]; then
    
      until kinit -kt /etc/security/keytabs/yarn.service.keytab yarn/hadoop@{{ .Values.realm }}; do sleep 15; done

      echo "KDC is up and ready to go... starting up"
      cp ${CONFIG_DIR}/start-yarn-rm.sh $HADOOP_HOME/sbin/
      cd $HADOOP_HOME/sbin
      chmod +x start-yarn-rm.sh
      ./start-yarn-rm.sh
    fi

    if [[ "${HOSTNAME}" =~ "yarn-nm" ]]; then
      sed -i '/<\/configuration>/d' $HADOOP_HOME/etc/hadoop/yarn-site.xml
      cat >> $HADOOP_HOME/etc/hadoop/yarn-site.xml <<- EOM
      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>${MY_MEM_LIMIT:-2048}</value>
      </property>

      <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>${MY_CPU_LIMIT:-2}</value>
      </property>
    EOM
      echo '</configuration>' >> $HADOOP_HOME/etc/hadoop/yarn-site.xml
      
      until kinit -kt /etc/security/keytabs/yarn.service.keytab yarn/hadoop@{{ .Values.realm }}; do sleep 15; done

      echo "KDC is up and ready to go... starting up"
      
      cp ${CONFIG_DIR}/start-yarn-nm.sh $HADOOP_HOME/sbin/
      cd $HADOOP_HOME/sbin
      chmod +x start-yarn-nm.sh

      #  wait up to 30 seconds for resourcemanager
      (while [[ $count -lt 15 && -z `curl -sf http://{{ include "hadoop.fullname" . }}-yarn-rm:8088/ws/v1/cluster/info` ]]; do ((count=count+1)) ; echo "Waiting for {{ include "hadoop.fullname" . }}-yarn-rm" ; sleep 2; done && [[ $count -lt 15 ]])
      [[ $? -ne 0 ]] && echo "Timeout waiting for yarn-rm, exiting." && exit 1

      ./start-yarn-nm.sh
    fi

    if [ ! -d "${HADOOP_HOME}/logs"  ]; then
       mkdir ${HADOOP_HOME}/logs
    fi

    if [[ $1 == "-d" ]]; then 
      until find ${HADOOP_HOME}/logs -mmin -1 | egrep -q '.*'; echo "`date`: Waiting for logs..." ; do sleep 2 ; done
      tail -F ${HADOOP_HOME}/logs/* &
      while true; do sleep 1000; done
    fi
    
    if [[ $1 == "-bash" ]]; then
      /bin/bash
    fi

  generate-jks.sh: |    
    FILE=/etc/security/keytabs/hadoop.jks
    if [ -f "$FILE" ]; then
        echo "hadoop.jks exist..using the existing file"
    else 
        echo "hadoop.jks does not exist..Creating a new file"
        keytool -genkey -alias hadoop-hadoop-hdfs-nn-0.{{ .Values.nn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-nn-0.{{ .Values.nn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-0.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-0.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-1.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-1.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-2.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-2.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-3.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-3.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-4.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-4.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-5.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-5.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-6.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-6.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-7.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-7.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-8.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-8.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-9.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-9.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-hdfs-dn-10.{{ .Values.dn_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-hdfs-dn-10.{{ .Values.dn_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-yarn-rm-0.{{ .Values.rm_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-yarn-rm-0.{{ .Values.rm_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-yarn-nm-0.{{ .Values.nm_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-yarn-nm-0.{{ .Values.nm_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hadoop-hadoop-yarn-nm-1.{{ .Values.nm_fqdn }} -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hadoop-hadoop-yarn-nm-1.{{ .Values.nm_fqdn }}" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
        keytool -genkey -alias hive-0.hive-service.tkg-bigdata-ns.svc.cluster.local -keyalg rsa -keysize 2048 -storetype PKCS12 -dname "CN=hive-0.hive-service.tkg-bigdata-ns.svc.cluster.local" -keypass bigdata -keystore /etc/security/keytabs/hadoop.jks -storepass bigdata
    fi
    
  core-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property><name>fs.defaultFS</name><value>hdfs://hadoop-hadoop-hdfs-nn-0.{{ .Values.nn_fqdn }}:9000</value></property>
      <property><name>hadoop.security.authorization</name><value>true</value></property>
      <property><name>hadoop.http.staticuser.user</name><value>yarn</value></property>
      <property><name>hadoop.proxyuser.hive.groups</name><value>*</value></property>
      <property><name>hadoop.proxyuser.hive.hosts</name><value>*</value></property>
      <property><name>hadoop.proxyuser.hdfs.groups</name><value>*</value></property>
      <property><name>hadoop.proxyuser.hdfs.hosts</name><value>*</value></property>
      <property><name>ipc.maximum.data.length</name><value>134217728</value></property>
      <property><name>hadoop.security.authentication</name><value>kerberos</value></property>
      <property><name>hadoop.security.authorization</name><value>true</value></property>
      <property><name>hadoop.rpc.protection</name><value>authentication</value></property>    
    </configuration>

  hdfs-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
    
{{- if .Values.hdfs.webhdfs.enabled -}}
      <property>
          <name>dfs.webhdfs.enabled</name>
          <value>true</value>
      </property> 
{{- end -}}
      <property>
        <name>dfs.client.datanode-restart.timeout</name>
        <value>30s</value>
      </property>
      <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>false</value>
      </property>

      <property>
        <name>dfs.replication</name>
          <value>3</value>
      </property>

      <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///dfs/data</value>
        <description>DataNode directory</description>
      </property>

      <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///dfs/name</value>
        <description>NameNode directory for namespace and transaction logs storage.</description>
      </property>

      <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
      </property>

      <!-- Bind to all interfaces -->
      <property>
        <name>dfs.namenode.rpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>dfs.namenode.servicerpc-bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->
      <property><name>ipc.maximum.data.length</name><value>134217728</value></property>
      <property><name>dfs.client.block.write.replace-datanode-on-failure.enable</name><value>true</value></property>
      <property><name>dfs.client.block.write.replace-datanode-on-failure.policy</name><value>ALWAYS</value></property>    
 
      <!-- General HDFS security config -->
      <property><name>dfs.permissions.enabled</name><value>true</value></property>
      <property><name>dfs.block.access.token.enable</name><value>true</value></property>
      
      <!-- NameNode security config -->
      <property><name>dfs.namenode.keytab.file</name><value>/etc/security/keytabs/hadoop.service.keytab</value></property>
      <property><name>dfs.namenode.kerberos.principal</name><value>hdfs/hadoop@{{ .Values.realm }}</value></property>
      <property><name>dfs.namenode.kerberos.internal.spnego.principal</name><value>HTTP/hadoop@{{ .Values.realm }}</value></property>
      
      <!-- For testing, we want tokens to expire FAST -->
      <property><name>dfs.namenode.delegation.token.max-lifetime</name><value>3600000</value><!-- 1 hour --></property>
      <property><name>dfs.namenode.delegation.token.renew-interval</name><value>3600000</value><!-- 1 hour --></property>
      
      <!-- DataNode security config -->
      <property><name>dfs.data.transfer.protection</name><value>integrity</value></property>
      <property><name>dfs.datanode.address</name><value>0.0.0.0:10019</value></property>
      <property><name>dfs.datanode.http.address</name><value>0.0.0.0:10022</value></property>
      <property><name>dfs.http.policy</name><value>HTTPS_ONLY</value></property>
      
      <!-- prevent those errors -->
      <property><name>dfs.namenode.datanode.registration.ip-hostname-check</name><value>false</value></property>
      <property><name>dfs.datanode.data.dir.perm</name><value>700</value></property>
      <property><name>dfs.data.dir</name><value>file:///dfs/data</value></property>
      <property><name>dfs.datanode.keytab.file</name><value>/etc/security/keytabs/hadoop.service.keytab</value> <!-- path to the HDFS keytab --></property>
      <property><name>dfs.datanode.kerberos.principal</name><value>hdfs/hadoop@{{ .Values.realm }}</value></property>
      <property><name>dfs.encrypt.data.transfer</name><value>true</value></property>
      <property><name>dfs.encrypt.data.transfer.cipher.suites</name><value>AES/CTR/NoPadding</value></property>
      <property><name>dfs.encrypt.data.transfer.cipher.key.bitlength</name><value>256</value></property>
      
      <!-- Web Authentication config -->
      
      <property><name>dfs.namenode.https-address</name>0.0.0.0:9871<value></value></property>
      <property><name>dfs.webhdfs.enabled</name><value>true</value></property>
      <property><name>dfs.web.authentication.kerberos.principal</name><value>HTTP/hadoop@{{ .Values.realm }}</value></property>
      <property><name>dfs.web.authentication.kerberos.keytab</name><value>/etc/security/keytabs/hadoop.service.keytab</value> <!-- path to the HTTP keytab --></property> 
    </configuration>

  mapred-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>mapreduce.jobhistory.address</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:10020</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:19888</value>
      </property>
      <property>
        <name>yarn.application.classpath</name>
        <value>
          /opt/hadoop/etc/*,
          /opt/hadoop/etc/hadoop/*,
          /opt/hadoop/lib/*,
          /opt/hadoop/lib/native/*,
          /opt/hadoop/share/hadoop/common/*,
          /opt/hadoop/share/hadoop/common/lib/*,
          /opt/hadoop/share/hadoop/hdfs/*,
          /opt/hadoop/share/hadoop/hdfs/lib/*,
          /opt/hadoop/share/hadoop/yarn/*,
          /opt/hadoop/share/hadoop/yarn/lib/*,
          /opt/hadoop/share/hadoop/mapreduce/*,
          /opt/hadoop/share/hadoop/mapreduce/lib/*,
          /opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar,
          /opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar
        </value>
      </property> 
      <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
      </property> 
    </configuration>

  slaves: |
    localhost

  start-yarn-nm.sh: |
    #!/usr/bin/env bash

    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.


    # Start all yarn daemons.  Run this on master node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    HADOOP_DEFAULT_LIBEXEC_DIR_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$HADOOP_DEFAULT_LIBEXEC_DIR_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    # start nodeManager
    $HADOOP_HOME/bin/yarn --daemon start nodemanager
    # if [[ whoami != yarn ]]
    # then 
       # echo "Switching to yarn user";
       # su yarn -c "$HADOOP_HOME/bin/yarn --daemon start nodemanager"
    # else
       # $HADOOP_HOME/bin/yarn --daemon start nodemanager
    # fi	

  start-yarn-rm.sh: |
    #!/usr/bin/env bash

    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.


    # Start all yarn daemons.  Run this on master node.

    echo "starting yarn daemons"

    bin=`dirname "${BASH_SOURCE-$0}"`
    bin=`cd "$bin"; pwd`

    HADOOP_DEFAULT_LIBEXEC_DIR_DIR="$bin"/../libexec
    HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$HADOOP_DEFAULT_LIBEXEC_DIR_DIR}
    . $HADOOP_LIBEXEC_DIR/yarn-config.sh

    #Starting Resource manager & Proxyserver
    $HADOOP_HOME/bin/yarn --daemon start resourcemanager
    $HADOOP_HOME/bin/yarn --daemon start proxyserver

    # if [[ whoami != yarn ]]
    # then 
       # echo "Switching to yarn user";
       # su yarn -c "$HADOOP_HOME/bin/yarn --daemon start resourcemanager"
       # su yarn -c "$HADOOP_HOME/bin/yarn --daemon start proxyserver"
    # else
       # $HADOOP_HOME/bin/yarn --daemon start resourcemanager
       # $HADOOP_HOME/bin/yarn --daemon start proxyserver
    # fi	

  yarn-site.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

    <configuration>
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm</value>
      </property>
      <!-- Bind to all interfaces -->
      <property>
        <description>Web Proxy Server</description>
        <name>yarn.web-proxy.address</name>
        <value>0.0.0.0:9046</value>
      </property>
      <property>
        <name>yarn.resourcemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.nodemanager.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <property>
        <name>yarn.timeline-service.bind-host</name>
        <value>0.0.0.0</value>
      </property>
      <!-- /Bind to all interfaces -->

      <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>

      <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
      </property>

      <property>
        <description>List of directories to store localized files in.</description>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/var/lib/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
      </property>

      <property>
        <description>Where to store container logs.</description>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/var/log/hadoop-yarn/containers</value>
      </property>

      <property>
        <description>Where to aggregate logs to.</description>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/var/log/hadoop-yarn/apps</value>
      </property>

      <property>
        <name>yarn.application.classpath</name>
        <value>
          /opt/hadoop/etc/hadoop,
          /opt/hadoop/share/hadoop/common/*,
          /opt/hadoop/share/hadoop/common/lib/*,
          /opt/hadoop/share/hadoop/hdfs/*,
          /opt/hadoop/share/hadoop/hdfs/lib/*,
          /opt/hadoop/share/hadoop/yarn/*,
          /opt/hadoop/share/hadoop/yarn/lib/*,
          /opt/hadoop/share/hadoop/mapreduce/*,
          /opt/hadoop/share/hadoop/mapreduce/lib/*,
          /opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.1.1.jar,
          /opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.1.1.jar
        </value>
      </property> 
      <property>
        <name>mapreduce.application.classpath</name>
        <value>
          /opt/hadoop/share/hadoop/mapreduce/lib/*,
          /opt/hadoop/share/hadoop/hdfs/*,
          /opt/hadoop/share/hadoop/hdfs/lib/*,
          /opt/hadoop/share/hadoop/common/lib/*,
          /opt/hadoop/share/hadoop/mapreduce/*,
          /opt/hadoop/share/hadoop/common/*,
          /opt/hadoop/share/hadoop/yarn/lib/*,
          /opt/hadoop/share/hadoop/yarn/*
        </value>
      </property>
      <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
      </property>
      <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
      </property>    
      <property>
        <name>yarn.log.server.url</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:19888/jobhistory/logs</value>
      </property>
      <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
      </property>
      <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>13312</value>
      </property>
      <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>2048</value>
      </property>
      <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>8192</value>
      </property>
      <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>2</value>
      </property>
       <property>
         <name>mapreduce.framework.name</name>
         <value>yarn</value>
       </property>
       <property>
       <name>yarn.resourcemanager.address</name>
       <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm</value>
       <description>Enter your ResourceManager hostname.</description>
       </property>
      <property>
       <name>yarn.resourcemanager.scheduler.address</name>
       <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm</value>
       <description>Enter your ResourceManager hostname.</description>
      </property>
      <property>
       <name>yarn.resourcemanager.resourcetracker.address</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm</value>
        <description>Enter your ResourceManager hostname.</description>
      </property>
     <property>
        <name>mapreduce.jobhistory.address</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:10020</value>
      </property>
      <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm.{{ .Release.Namespace }}.svc.cluster.local:19888</value>
      </property>
      <property>
       <name>yarn.resourcemanager.resourcetracker.address</name>
        <value>{{ include "hadoop.fullname" . }}-yarn-rm-0.{{ include "hadoop.fullname" . }}-yarn-rm</value>
      </property>
      <property>
       <name>yarn.resourcemanager.scheduler.class</name>
       <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
      </property>
      <property><name>yarn.nodemanager.log-dirs</name><value>/opt/hadoop/logs</value></property>
      
      <!-- Resource Manager Kerberos Security Settings -->
      <property><name>yarn.resourcemanager.principal</name><value>yarn/hadoop@{{ .Values.realm }}</value></property>             
      <property><name>yarn.resourcemanager.keytab</name><value>/etc/security/keytabs/yarn.service.keytab</value></property>
      <property><name>yarn.resourcemanager.webapp.https.address</name><value>0.0.0.0:8090</value></property>
      
      <!-- Node Manager Kerberos Security Settings -->
      <property><name>yarn.nodemanager.principal</name><value>yarn/hadoop@{{ .Values.realm }}</value></property>
      <property><name>yarn.nodemanager.keytab</name><value>/etc/security/keytabs/yarn.service.keytab</value></property>
      <property><name>yarn.nodemanager.webapp.https.address</name><value>0.0.0.0:8044</value></property>

      <!-- WebApp Proxy Kerberos Security Settings -->
      <property><name>yarn.web-proxy.principal</name><value>HTTP/hadoop@{{ .Values.realm }}</value></property>
      <property><name>yarn.web-proxy.keytab</name><value>/etc/security/keytabs/yarn.service.keytab</value></property>    

      <!-- MapReduce JobHistory Server Kerberos Security Settings -->
      <property><name>mapreduce.jobhistory.principal</name><value>HTTP/hadoop@{{ .Values.realm }}</value></property>
      <property><name>mapreduce.jobhistory.keytab</name><value>/etc/security/keytabs/yarn.service.keytab</value></property>
      
    </configuration>
    
  capacity-scheduler.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
      <property>
        <name>yarn.scheduler.capacity.root.queues</name>
        <value>default,compaction,dataengineering</value>
        <description>
          The queues at the this level (root is the root queue).
        </description>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.default.capacity</name>
        <value>20</value>
        <description>Compaction queue target capacity.</description>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.compaction.capacity</name>
        <value>40</value>
        <description>Compaction queue target capacity.</description>
      </property>
      <property>
        <name>yarn.scheduler.capacity.root.dataengineering.capacity</name>
        <value>40</value>
        <description>DataEngineering queue target capacity.</description>
      </property>
      <property>
        <name>yarn.scheduler.capacity.queue-mappings</name>
        <value>u:hive:compaction,g:hdpuser:dataengineering</value>
        <description>
          A list of mappings that will be used to assign jobs to queues
          The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*
          Typically this list will be used to map users to queues,
          for example, u:%user:%user maps all users to queues with the same name
          as the user.
        </description>
      </property> 
    </configuration>
  krb5.conf: |
    # To opt out of the system crypto-policies configuration of krb5, remove the
    # symlink at /etc/krb5.conf.d/crypto-policies which will not be recreated.
    includedir /etc/krb5.conf.d/
    
    [logging]
        default = FILE:/var/log/krb5libs.log
        kdc = FILE:/var/log/krb5kdc.log
        admin_server = FILE:/var/log/kadmind.log
    
    [libdefaults]
        dns_lookup_realm = true
        dns_lookup_kdc = true
        ticket_lifetime = 24h
        renew_lifetime = 7d
        forwardable = true
        rdns = false
        pkinit_anchors = FILE:/etc/pki/tls/certs/ca-bundle.crt
        spake_preauth_groups = edwards25519
        default_realm = {{ .Values.realm }}
        # default_ccache_name = KEYRING:persistent:%{uid}
    
    [realms]
     {{ .Values.realm }} = {
         kdc = bigdata-krb-01.blueprint.lab
         admin_server = bigdata-krb-01.blueprint.lab
     }
    
    [domain_realm]
     .{{ .Values.realm }} = {{ .Values.realm }}
     {{ .Values.realm }} = {{ .Values.realm }}
 
  ssl-server.xml: |
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <!--
       Licensed to the Apache Software Foundation (ASF) under one or more
       contributor license agreements.  See the NOTICE file distributed with
       this work for additional information regarding copyright ownership.
       The ASF licenses this file to You under the Apache License, Version 2.0
       (the "License"); you may not use this file except in compliance with
       the License.  You may obtain a copy of the License at
    
           http://www.apache.org/licenses/LICENSE-2.0
    
       Unless required by applicable law or agreed to in writing, software
       distributed under the License is distributed on an "AS IS" BASIS,
       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       See the License for the specific language governing permissions and
       limitations under the License.
    -->
    <configuration>
    <property><name>ssl.server.truststore.location</name><value>/etc/security/keytabs/hadoop.jks</value></property>
    <property><name>ssl.server.truststore.password</name><value>bigdata</value></property>
    <property><name>ssl.server.keystore.location</name><value>/etc/security/keytabs/hadoop.jks</value></property>
    <property><name>ssl.server.keystore.password</name><value>bigdata</value></property>
    <property><name>ssl.server.keystore.keypassword</name><value>bigdata</value></property>
    </configuration>

  hosts: |
    # Kubernetes-managed hosts file.
    127.0.0.1       localhost
    ::1     localhost ip6-localhost ip6-loopback
    fe00::0 ip6-localnet
    fe00::0 ip6-mcastprefix
    fe00::1 ip6-allnodes
    fe00::2 ip6-allrouters
    192.0.22.125    hadoop-hadoop-hdfs-nn-0.{{ .Values.nn_fqdn }}  hadoop-hadoop-hdfs-nn-0
    192.0.22.126    DN_HOST.{{ .Values.dn_fqdn }}  DN_HOST