# The base hadoop image to use for all components.
# See this repo for image build details: https://github.com/Comcast/kube-yarn/tree/master/image
image:
  repository: danisla/hadoop
  tag: 2.8.3
  pullPolicy: Always

# The version of the hadoop libraries being used in the image.
hadoopVersion: 2.8.3

# Select antiAffinity as either hard or soft, default is soft
antiAffinity: "soft"

hdfs:
  nameNode:
    pdbMinAvailable: 1

    # resources:
    #   requests:
    #     memory: "256Mi"
    #     cpu: "10m"
    #   limits:
    #     memory: "2048Mi"
    #     cpu: "1000m"

  dataNode:
    replicas: 1

    pdbMinAvailable: 1

    # resources:
    #   requests:
    #     memory: "256Mi"
    #     cpu: "10m"
    #   limits:
    #     memory: "2048Mi"
    #     cpu: "1000m"

  webhdfs:
    enabled: true

yarn:
  resourceManager:
    pdbMinAvailable: 1

    # resources:
    #   requests:
    #     memory: "256Mi"
    #     cpu: "10m"
    #   limits:
    #     memory: "2048Mi"
    #     cpu: "2000m"

  nodeManager:
    pdbMinAvailable: 1

    # The number of YARN NodeManager instances.
    replicas: 1

    # Create statefulsets in parallel (K8S 1.7+)
    parallelCreate: false

    # CPU and memory resources allocated to each node manager pod.
    # This should be tuned to fit your workload.
    # resources:
    #   requests:
    #     memory: "2048Mi"
    #     cpu: "1000m"
    #   limits:
    #     memory: "2048Mi"
    #     cpu: "1000m"

persistence:
  nameNode:
    enabled: true
    storageClass: "vsphere-with-tanzu-storage-policy"
    accessMode: ReadWriteOnce
    size: 5Gi

  dataNode:
    enabled: true
    storageClass: "vsphere-with-tanzu-storage-policy"
    accessMode: ReadWriteOnce
    size: 20Gi

# 1. You can check the status of HDFS by running this command:
   # kubectl exec -n tkg-bigdata-ns -it hadoop-hadoop-hdfs-nn-0 -- /usr/local/hadoop/bin/hdfs dfsadmin -report

# 2. You can list the yarn nodes by running this command:
   # kubectl exec -n tkg-bigdata-ns -it hadoop-hadoop-yarn-rm-0 -- /usr/local/hadoop/bin/yarn node -list

# 3. Create a port-forward to the yarn resource manager UI:
   # kubectl port-forward -n tkg-bigdata-ns hadoop-hadoop-yarn-rm-0 8088:8088

   # Then open the ui in your browser:

   # open http://localhost:8088

# 4. You can run included hadoop tests like this:
   # kubectl exec -n tkg-bigdata-ns -it hadoop-hadoop-yarn-nm-0 -- /usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.3-tests.jar TestDFSIO -write -nrFiles 5 -fileSize 128MB -resFile /tmp/TestDFSIOwrite.txt

# 5. You can list the mapreduce jobs like this:
   # kubectl exec -n tkg-bigdata-ns -it hadoop-hadoop-yarn-rm-0 -- /usr/local/hadoop/bin/mapred job -list

# 6. This chart can also be used with the zeppelin chart
    # helm install --namespace tkg-bigdata-ns --set hadoop.useConfigMap=true,hadoop.configMapName=hadoop-hadoop stable/zeppelin

# 7. You can scale the number of yarn nodes like this:
   # helm upgrade hadoop --set yarn.nodeManager.replicas=4 stable/hadoop