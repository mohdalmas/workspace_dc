  
image:
  repository: et02-harbor.safaricomet.net/bigdata/hive-spark
  tag: hive-3.1.0-spark-2.3.0-scala-2.11.6-kms-test
  pullPolicy: IfNotPresent

serviceAccount: bigdata

# Select antiAffinity as either hard or soft, default is soft
antiAffinity: "soft"

imagePullSecrets:
- name: regcred

# tolerations:
# - key: "dedicated"
  # operator: "Equal"
  # value: "Hadoop"
  # effect: "NoSchedule"
  
# affinity:
  # nodeAffinity:
    # requiredDuringSchedulingIgnoredDuringExecution:
      # nodeSelectorTerms:
      # - matchExpressions:
        # - key: dedicated
          # operator: In
          # values:
          # - Hadoop

spark_master:
  resources: 
    requests:
      memory: "1Gi"
      cpu: "1"
    limits:
      memory: "2Gi"
      cpu: "1"
  
spark_worker:
  replicas: 1
  resources: 
    requests:
      memory: "3Gi"
      cpu: "2"
    limits:
      memory: "14Gi"
      cpu: "7"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 8
    metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80


spark:
  master:
    url: spark://spark-master-0.spark-headless:7077
    host: spark-master-0.spark-headless
  worker:
    executioncores: "6"
    executionmemory: "14G"

## @section Traffic Exposure parameters

## Service parameters
##
service:
  ## @param service.type Kubernetes Service type
  ##
  type: ClusterIP
  ## @param service.loadBalancerIP Load balancer IP if spark service type is `LoadBalancer`
  ## Set the LoadBalancer service type to internal only
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
  ##
  loadBalancerIP: ""

## Configure the ingress resource that allows you to access the
## Spark installation. Set up the URL
## ref: https://kubernetes.io/docs/user-guide/ingress/
##
ingress:

  user: spark
  password: nNgmCcY4zmD3LJxE
  ## @param ingress.enabled Enable ingress controller resource
  ##
  enabled: true
  ## DEPRECATED: Use ingress.annotations instead of ingress.certManager
  ## certManager: false
  ##

  ## @param ingress.pathType Ingress path type
  ##
  pathType: ImplementationSpecific
  ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
  ##
  apiVersion: ""
  ## @param ingress.hostname Default host for the ingress resource
  ##
  hostname: spark.big-data2.safaricomet.net
  ## @param ingress.path The Path to Spark. You may need to set this to '/*' in order to use this with ALB ingress controllers.
  ##
  hostPaths:
  - host: spark-sm.big-data2.safaricomet.net
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: spark-master-service
            port:
              number: 8080
  - host: spark-sw.big-data2.safaricomet.net
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: spark-worker-service
            port:
              number: 8081
  - host: spark-wd.big-data2.safaricomet.net
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: spark-worker-service
            port:
              number: 4040

    
    
conf:
  hadoopConfigMap: hadoop-hadoop
  hiveSite:
    datanucleus.autoCreateSchema: false                                          
    hive.exec.scratchdir: /tmp/
    hive.exec.stagingdir: /tmp/
    hive.execution.engine: spark
    hive.execution.mode: container
    hive.metastore.event.db.notification.api.auth: false
    hive.metastore.schema.verification: false
    hive.metastore.schema.verification.record.version: false
    hive.metastore.uris: thrift://hive-0.hive-service:9083
    hive.metastore.warehouse.dir: hdfs://hadoop-hadoop-hdfs-nn:9000/hive/warehouse
    hive.server2.enable.doAs: false
    hive.server2.zookeeper.namespace: hiveserver2
    hive.txn.manager: org.apache.hadoop.hive.ql.lockmgr.DbTxnManager
    hive.zookeeper.client.port: "2181"
    hive.zookeeper.quorum: bigdata-zk-zookeeper-0.bigdata-zk-zookeeper-headless:2181,bigdata-zk-zookeeper-1.bigdata-zk-zookeeper-headless:2181,bigdata-zk-zookeeper-2.bigdata-zk-zookeeper-headless:2181
    javax.jdo.option.ConnectionDriverName: org.postgresql.Driver
    javax.jdo.option.ConnectionPassword: hive
    javax.jdo.option.ConnectionURL: jdbc:postgresql://bigdata-psql-postgresql-headless/metastore
    javax.jdo.option.ConnectionUserName: hive
    hive.support.concurrency: true
    hive.exec.reducers.bytes.per.reducer: "256000000"
    hive.exec.reducers.max: "1009"
    hive.compactor.initiator.on: true
    hive.enforce.bucketing: true
    hive.exec.dynamic.partition.mode: nonstrict
    hive.spark.client.channel.log.level: ERROR
    hive.spark.client.connect.timeout: 30000ms
    hive.spark.client.rpc.server.address: hive-0.hive-service
    hive.spark.client.server.connect.timeout: 1000000ms
    hive.spark.job.monitor.timeout: 360s
    hive.stats.jdbc.timeout: 30
    spark.eventLog.dir: /var/log/spark
    spark.eventLog.enabled: false
    spark.master: spark://spark-master-0.spark-headless:7077
    spark.serializer: org.apache.spark.serializer.KryoSerializer
    hive.compactor.initiator.on: true
    hive.compactor.delta.num.threshold: 2
    hive.compactor.delta.pct.threshold: "0.1f"
    hive.txn.strict.locking.mode: true
    hive.txn.retryable.sqlex.regex: ""
    NO_AUTO_COMPACTION: false
    hive.compactor.max.num.delta: 10
    hive.compactor.check.interval: 300
    hive.log.explain.output: true
    hive.metastore.runworker.in: hs2
    hive.compactor.worker.threads: 1
    hive.compactor.job.queue: compaction
    hive.stats.column.autogather: false
    hive.optimize.sort.dynamic.partition: true
    hive.server2.session.check.interval: "300000" # 5 minutes 
    hive.server2.idle.operation.timeout: "480000" # 8 minutes
    hive.server2.idle.session.timeout: "600000" # 10 minutes
    spark.driver.memory: 1024m
    spark.executor.memory: 3072m
    spark.cores.max: 2
    spark.driver.cores: 1
    spark.executor.cores: 2
    spark.default.parallelism: 7 
    hive.server2.authentication: NOSASL
    hive.server2.transport.mode: http
    hive.server2.thrift.http.port: 10000
    hive.server2.thrift.http.path:  cliservice
    kms.api.auth.password: lyOdmSBwCsZnD4dLnOAE
    kms.api.auth.username: svc-bigdata-admin
    kms.api.url: http://10.3.41.29:8080
    
  spark-Defaults:
    spark.master.rest.enabled: false
    spark.sql.hive.hiveserver2.jdbc.url: "jdbc:hive2://hive-0.hive-service:10000"
    spark.datasource.hive.warehouse.metastoreUri: "thrift://hive-0.hive-service:9083"
    spark.hadoop.hive.zookeeper.quorum: "bigdata-zk-zookeeper-0.bigdata-zk-zookeeper-headless:2181,bigdata-zk-zookeeper-1.bigdata-zk-zookeeper-headless:2181,bigdata-zk-zookeeper-2.bigdata-zk-zookeeper-headless:2181"
    spark.master: "spark://spark-master-0.spark-headless:7077"
    spark.eventLog.enabled: false
    spark.eventLog.dir: /var/log/spark
    spark.serializer: org.apache.spark.serializer.KryoSerializer
    spark.executor.extraJavaOptions: -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
    spark.yarn.submit.waitAppCompletion: true
    spark.driver.memory: 1024m
    spark.executor.memory: 3072m
    spark.cores.max: 2
    spark.driver.cores: 1
    spark.executor.cores: 2
    spark.default.parallelism: 7  
    spark.dynamicAllocation.enabled: false 
    spark.dynamicAllocation.executorIdleTimeout: 60s
    spark.dynamicAllocation.minExecutors: 0
    spark.worker.memory: 11g
    # kms.api.auth.password: lyOdmSBwCsZnD4dLnOAE
    # kms.api.auth.username: svc-bigdata-admin
    # kms.api.url: http://10.3.41.29:8080
